<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="files/jemdoc.css" type="text/css" />

<!--<link rel="shortcut icon" href="./files/icon.ico">-->
<title>李士刚 北京邮电大学, Homepage of Shigang Li @ BUPT</title>
</head>
 
 
<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 


<class="staffshortcut">
 </font><font size="5"> 
 <A HREF="#Publications">Publications</A> |
 <A HREF="#Talks">Talks</A> |
 <A HREF="#Teaching">Teaching</A> |
 <A HREF="#Academic Services">Services</A> |
 <A HREF="#People">People</A> |
 <A HREF="#Position Openings">Position Openings</A>
 </font>
<br />
<br />

<table class="imgtable"><tr><td>
<a href="./"><img src="./files/photo2.jpg" alt="" height="211px" /></a>&nbsp;</td>
		<td align="left"><p><font size="5"><b>李士刚 &nbsp; 北京邮电大学 </b></font><font size="5"; font style="font-family:Sun"> | Shigang Li @ BUPT</font><br />

</font><font size="4">
	<b><font color="#002366"> Professor, PhD Supervisor</font></b>, <b><font color="#002366"> Director of ParCIS Lab, Beijing University of Posts and Telecommunications</b> <br />
<!--<b>Official Homepage: &nbsp; &nbsp; </b> <br />-->
<b>Links: &nbsp; &nbsp; </b>[<a href="https://scholar.google.com.hk/citations?user=KEUan7IAAAAJ&hl=en" target="_blank">Google Scholar</a>] &nbsp; [<a href="https://www.researchgate.net/profile/Shigang_Li2" target="_blank">ResearchGate</a>] &nbsp; <!--[<a href="https://www.linkedin.com/in/shigang-li-87aa1458/" target="_blank">LinkedIn</a>] &nbsp;--> [<a href="https://github.com/ParCIS" target="_blank">GitHub</a>] &nbsp; [<a href="https://orcid.org/0000-0003-0022-7865" target="_blank">ORCID</a>] <br /> <br />
<b>Research interests: </b> &nbsp; Parallel Computing, &nbsp; Deep Learning Systems, &nbsp; High-Performance Computing, &nbsp; Computer Architecture, &nbsp; Heterogeneous Computing <br />
<b>Emails: &nbsp; </b> <a href="">shigangli.cs@gmail.com</a>; &nbsp; <a href="">lishigang@bupt.edu.cn</a> <br />
</font>

</p>
</td></tr></table>


<A NAME="Brief Biography"><h2>Brief Biography</h2></A>
</font><font size="4">
	&nbsp; &nbsp; &nbsp; &nbsp; Dr. Shigang Li is currently a Professor in School of Computer Science, <a href="">Beijing University of Posts and Telecommunications</a>, where he is leading the <a href="">Parallel Computing and Intelligent Systems Laboratory</a> (<b>ParCIS Lab</b>). His research interests include parallel and distributed deep learning systems, high performance computing, and heterogeneous computing. He was a Postdoctoral Researcher in <a href="">SPCL Lab, ETH Zurich</a> from Aug. 2018 to Aug. 2022. He received the Bachelor's degree majored in Computer Science and the Ph.D degree majored in Computer Architecture from University of Science and Technology Beijing, in 2009 and 2014, respectively. He has been a joint Ph.D student in Department of Computer Science, University of Illinois at Urbana-Champaign from Sep. 2011 to Sep. 2013. He has been an Assistant Professor in State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences from 2014 to 2018. He got the Best Paper Nominations (as the leading author) in SC'22, SC'21, PPoPP'20 and HPDC'13, and Outstanding Paper Award of MLSys'21, Best Reproducibility Advancement Award of SC'22. He has served as the PC members in top conferences (SC, PPoPP, IPDPS, IEEE Cluster, ICPP, etc.) and the invited reviewers in prestigious journals (IEEE TPDS, IEEE TSC, IEEE TBD, JPDC, etc.). He is the Associate Editor of Cluster Computing and the Youth Editor of CCF THPC, and has been Publicity Co-Chair of PPoPP'23, Publications Chair of IISWC'20, and Workshop Co-Chair of ICS'18. He is an Executive Committee Member of CCF TCHPC, and an Executive Committee Member of ACM SIGHPC China Chapter. He is a senior member of IEEE, ACM and CCF. <br />


<A NAME="Position Openings"><h2>Position Openings</h2></A>
<table class="imgtable"><tr><td>
<a href="./"><img src="./files/logo.jpg" alt="" height="60px" /></a>&nbsp;</td>
		<td align="left"><font size="4"> 
	&nbsp; &nbsp; &nbsp; &nbsp; I'm leading the Parallel Computing and Intelligent Systems Laboratory (ParCIS Lab) in BUPT, and <font color="blue"> <b>we are looking for highly self-motivated PhD and Master students, Postdocs, and higher-level talents</b></font>. Let's work together on HPC+AI and make something cool! Contact me directly with your CV if you're interested.
</font>
</td></tr></table>

<!--
<A NAME="News"><h2>News</h2></A>
<ul>
</font><font size="4">
<li> <b> <font color="#002366">[Aug. 13, 2022]</font> </b>, I'm leading the Parallel Computing and Systems Laboratory in BUPT, and <font color="blue">we are recruiting Master and PhD students, and Postdocs</font>. Let's work together on HPC+AI! Send me email if you're interested. </li>
<li> <b> <font color="#002366">[Sept. 20, 2022]</font> </b>, talk in <a href= "http://nicsefc.ee.tsinghua.edu.cn/detail.html?id=1030">ARC 2022</a> &#151; Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. </li>
<li> <b> <font color="#002366">[July 14, 2022]</font> </b>, very happy that Magicube and Chimera (as the leading author) were nominated for <font color="#FF0000">Best Paper in SC</font> for <font color="#FF0000">two consecutive years</font> (<a href= "https://sc22.supercomputing.org/">SC'22</a> and <a href= "https://sc21.supercomputing.org/">SC'21</a>). Will continue to move on :-) </li>
<li> <b> <font color="#002366">[June 15, 2022]</font> </b>, very glad that <font color="blue">TWO papers are accepted by</font> <a href= "https://sc22.supercomputing.org/">SC 2022</a>. A thrilling story is that one paper was submitted in the last 10 seconds before the deadline. I would like to say don't rush to submit until the paper is good enough, but 10 seconds is definitely not elegant :-) </li>
<li> <b> <font color="#002366">[June 10, 2022]</font> </b>, very glad to serve as Publicity Chair (Europe) of <a href= "https://ppopp23.sigplan.org/">PPoPP 2023</a>. See call for papers <a href= "https://ppopp23.sigplan.org/track/PPoPP-2023-papers#Call-for-Papers">here</a>.</li>
<li> <b> <font color="#002366">[April 4, 2022]</font> </b>, talk in <a href= "https://ppopp22.sigplan.org/">PPoPP'22</a> &#151; Near-Optimal Sparse Allreduce for Distributed Deep Learning. [<a href= "https://www.youtube.com/watch?v=Req6Cs17ur0">Video</a>]</li>
<li> <b> <font color="#002366">[Nov. 16, 2021]</font> </b>, talk in <a href= "https://sc21.supercomputing.org/program/papers/">SC'21</a> &#151; Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. [<a href= "https://dl.acm.org/doi/abs/10.1145/3458817.3476145#sec-supp">Video</a>]</li>
<li> <b> <font color="#002366">[Aug. 28, 2021]</font> </b>, talk in <a href= "https://www.emc2-ai.org/virtual-21">ECM2'21</a> &#151; Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. [<a href= "https://www.youtube.com/watch?v=0ydk8u8ipmM">Video</a>]</li>
<li> <b> <font color="#002366">[Feb. 24, 2020]</font> </b>, talk in <a href= "https://ppopp20.sigplan.org/">PPoPP'20</a> &#151; Taming unbalanced training workloads in deep learning with partial collective operations. [<a href= "https://www.youtube.com/watch?v=CvoreY0Guws&t=0s">Video</a>]</li>
<li> <b> <font color="#002366">[Oct. 26, 2016]</font> </b>, talk in <a href= "https://sites.google.com/a/lbl.gov/padal-workshop/previous-padals/padal16/schedule">PADAL'16</a> &#151; Cache-Oblivious MPI All-to-All Collectives. [<a href= "https://www.youtube.com/watch?v=w52qBZ2sSoE">Video</a>]</li>
<li> <b> <font color="#FF0000">[2020.10]</font> </b> I'm playing a very interesting newly discovered Jigsaw Puzzle game :-) It's super cool! </li>
</font>
</ul>
-->

<A NAME="Talks"><h2>Talks</h2></A>
<font size="4"> 
<ul>
<li> <b><font color="#002366">Dec. 10, 2024</font></b> &#151; <a href= "">2024 Computing Foundation Software Scholars Committee Workshop, Beijing</a> &#151; Acceleration for Unstructured Sparse Matrix Operators. </li>
<li> <b><font color="#002366">Nov. 30, 2024</font></b> &#151; <a href= "https://ccf.org.cn/chinastorage">CCF China Storage Conference, Forum of Large Model Training and Parallel Processing, Guangzhou</a> &#151; Accelerating Unstructured Sparsity on Tensor Cores for Large Models. </li>
<li> <b><font color="#002366">Oct. 28, 2024</font></b> &#151; <a href= "">Seminar on AI Large Model Reasoning, Beijing</a> &#151; Parallel and Distributed Inference for Sparse Large Models. </li>
<li> <b><font color="#002366">Aug. 11, 2024</font></b> &#151; <a href= "https://c2sml.cn/index_en.html">CSML 2024, Shanghai</a> &#151; Research on Efficient and Scalable Parallel Strategies for Large Models. </li>
<li> <b><font color="#002366">June 15, 2024</font></b> &#151; <a href= "https://2024.baai.ac.cn/schedule">BAAI Conference 2024, AI System Forum, Beijing</a> &#151; Research on Efficient and Scalable Parallel Strategies for Large Models. </li>
<li> <b><font color="#002366">May 21, 2024</font></b> &#151; <a href= "">STW 2024, Shenzhen</a> &#151; Exploring scalable and efficient parallel schemes for large models. </li>
<li> <b><font color="#002366">Nov. 25, 2023</font></b> &#151; <a href= "">Semiconductor Technology Forum, 2023, Beijing</a> &#151; Integrating Fisher Information Matrix with Pipeline Parallelism for Large Model Training. </li>
<li> <b><font color="#002366">Aug. 26, 2023</font></b> &#151; <a href= "https://hpcchina.ccf.org.cn/">CCF HPC China 2023, Forum on Compiler, Runtime, and Performance Optimization for Diverse Computing Power, Qingdao</a> &#151; Integrating Fisher Information Matrix with Pipeline Parallelism for Large Model Training. </li>
<li> <b><font color="#002366">Mar. 9, 2023</font></b> &#151; <a href= "https://www.ee.tsinghua.edu.cn/info/1076/3986.htm">The Second Workshop for Advanced Computing, Beijing</a> &#151; Magicube: Efficient Quantized Sparse Matrix Operations on Tensor Cores. </li>
<li> <b><font color="#002366">Dec. 12, 2022</font></b> &#151; <a href= "https://hpcchina.ccf.org.cn/">CCF HPC China 2022, the First Forum for High-Performance Deep Learning Systems</a> &#151; Challenges of High-Performance Deep Learning Systems and the Countermeasures. </li>
<li> <b><font color="#002366">Dec. 12, 2022</font></b> &#151; <a href= "http://www.caep-scns.ac.cn/HPCMid-2022-775.php">HPCMid-2022</a> &#151; Efficient Quantized Sparse Matrix Operations on Tensor Cores. </li>
<li> <b><font color="#002366">Dec. 15, 2022</font></b> &#151; <a href= "https://hpcchina.ccf.org.cn/">CCF HPC China 2022, the Fourth Forum for Architectures, Algorithms, and Applications of High-Performance Sparse Matrix Computation</a> &#151; Efficient Quantized Sparse Matrix Operations on Tensor Cores. </li>
<li> <b><font color="#002366">Dec. 15, 2022</font></b> &#151; <a href= "https://hpcchina.ccf.org.cn/">CCF HPC China 2022, the Sixth Forum for HPC Performance Model</a> &#151; 大模型双向流水线并行系统Chimera. </li>
<li> <b><font color="#002366">Sept. 20, 2022</font></b> &#151; <a href= "http://nicsefc.ee.tsinghua.edu.cn/detail.html?id=1030">The 18th International Symposium on Applied Reconfigurable Computing (ARC 2022), Beijing</a> &#151; Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. </li>
<li> <b><font color="#002366">April 4, 2022</font></b> &#151; <a href= "https://ppopp22.sigplan.org/">PPoPP'22</a> &#151; Near-Optimal Sparse Allreduce for Distributed Deep Learning. [<a href= "https://www.youtube.com/watch?v=Req6Cs17ur0">Video</a>]</li>
<li> <b><font color="#002366">Nov. 16, 2021</font></b> &#151; <a href= "https://sc21.supercomputing.org/program/papers/">SC'21</a> &#151; Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. [<a href= "https://dl.acm.org/doi/abs/10.1145/3458817.3476145#sec-supp">Video</a>]</li>
<li> <b><font color="#002366">Aug. 28, 2021</font></b> &#151; <a href= "https://www.emc2-ai.org/virtual-21">ECM2'21</a> &#151; Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. [<a href= "https://www.youtube.com/watch?v=0ydk8u8ipmM">Video</a>]</li>
<li> <b><font color="#002366">Feb. 24, 2020</font></b> &#151; <a href= "https://ppopp20.sigplan.org/">PPoPP'20, San Diego</a> &#151; Taming unbalanced training workloads in deep learning with partial collective operations. [<a href= "https://www.youtube.com/watch?v=CvoreY0Guws&t=0s">Video</a>]</li>
<li> <b><font color="#002366">Oct. 26, 2016</font></b> &#151; <a href= "https://sites.google.com/a/lbl.gov/padal-workshop/previous-padals/padal16/schedule">PADAL'16, Kobe</a> &#151; Cache-Oblivious MPI All-to-All Collectives. [<a href= "https://www.youtube.com/watch?v=w52qBZ2sSoE">Video</a>]</li>
</ul>
</font>


<A NAME="Publications"><h2>Publications</h2></A>
<ul>
</font><font size="4">

<li>
<span><b><font color="#002366">[SC'2025]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Jingkun Dong, Jiahao Chen, Zhi Ma, Zhongzhe Hu, et al. Hypertron: Efficiently Scaling Large Models by Exploring High-Dimensional Parallelization Space. The International Conference for High Performance Computing, Networking, Storage and Analysis, ACM, 2025.
</span>
</li>

<li>
<span><b><font color="#002366">[ICS'2025]</font></b> &nbsp; Lixing Zhang, Yingxia Shao <b><font color="#002366">Shigang Li</font></b>. CoLa: Towards Communication-Efficient Distributed Sparse Matrix-Matrix Multiplication on GPUs. The 39th ACM International Conference on Supercomputing, 2025. [<a href= "">Paper</a>]
</span>
</li>
	
<li>
<span><b><font color="#002366">[TACO'2025]</font></b> &nbsp; Xueying Wang, <b><font color="#002366">Shigang Li*</font></b>, Hao Qian, et al. OptiFX: Automatic Optimization for Convolutional Neural Networks with Aggressive Operator Fusion on GPUs. ACM Transactions on Architecture and Code Optimization, 2025. [<a href= "https://dl.acm.org/doi/pdf/10.1145/3716876">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[CCF THPC'2025]</font></b> &nbsp; Youxuan Xu, Tong Wu, <b><font color="#002366">Shigang Li*</font></b>, Xueying Wang, Jingjing Wang. SparkAttention: High-Performance Multi-Head Attention for Large Models on Volta GPU Architecture. CCF Transactions on High Performance Computing, 2025. [<a href= "https://arxiv.org/pdf/2502.12784">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[PPoPP'2025]</font></b> &nbsp; Jinliang Shi, <b><font color="#002366">Shigang Li*</font></b>, Youxuan Xu, Rongtian Fu, Xueying Wang, Tong Wu. FlashSparse: Minimizing Computation Redundancy for Fast Sparse Matrix Multiplications on Tensor Cores. The 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, 2025. [<a href= "https://dl.acm.org/doi/pdf/10.1145/3710848.3710858">Paper</a>][<a href= "https://github.com/ParCIS/FlashSparse">Code</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[TPDS'2024]</font></b> &nbsp; Jinfan Chen, <b><font color="#002366">Shigang Li*</font></b>, Ran Guo, Jinhui Yuan, Torsten Hoefler. AutoDDL: Automatic Distributed Deep Learning with Near-Optimal Bandwidth Cost. IEEE Transactions on Parallel and Distributed Systems (2024). [<a href= "https://ieeexplore.ieee.org/document/10521778">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[CACM'2024]</font></b> &nbsp; Torsten Hoefler, Tommaso Bonato, Daniele De Sensi, Salvatore Di Girolamo, <b><font color="#002366">Shigang Li</font></b>, Marco Heddes, Deepak Goel, Miguel Castro, Steve Scott. HammingMesh: A Network Topology for Large-Scale Deep Learning. Communications of the ACM, Volume 67, Issue 12, Pages 97-105, 2024. [<a href= "https://dl.acm.org/doi/pdf/10.1145/3623490">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[NSDI'2024]</font></b> &nbsp; Nils Blach, Maciej Besta, Daniele De Sensi, Jens Domke, Hussein Harake, <b><font color="#002366">Shigang Li</font></b>, Patrick Iff, Marek Konieczny, Kartik Lakhotia, Ales Kubicek, Marcel Ferrari, Fabrizio Petrini, Torsten Hoefler. A High-Performance Design, Implementation, Deployment, and Evaluation of The Slim Fly Network. The 20th USENIX Symposium on Networked Systems Design and Implementation, 2024. [<a href= "https://www.usenix.org/system/files/nsdi24-blach.pdf">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[PPoPP'2024]</font></b> &nbsp; Shunde Li, Junyu Gu, Jue Wang, Tiechui Yao, Zhiqiang Liang, Yumeng Shi, <b><font color="#002366">Shigang Li</font></b>, Weiting Xi, Shushen Li, Chunbao Zhou, Yangang Wang, Xuebin Chi. ParGNN: Efficient Training for Large-Scale Graph Neural Network on GPU Clusters. Poster. In Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, 2024. [<a href= "https://dl.acm.org/doi/abs/10.1145/3627535.3638488">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[TPDS'2023]</font></b> &nbsp; Hang Cao, Liang Yuan, He Zhang, Yunquan Zhang, Baodong Wu, Kun Li, <b><font color="#002366">Shigang Li</font></b>, Minghua Zhang, Pengqi Lu, Junmin Xiao. AGCM-3DLF: Accelerating Atmospheric General Circulation Model via 3D Parallelization and Leap-Format. IEEE Transactions on Parallel and Distributed Systems (2023). [<a href= "https://ieeexplore.ieee.org/abstract/document/9996169/">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[SC'2023]</font></b> &nbsp; Wenqi Jiang, <b><font color="#002366">Shigang Li</font></b>, Yu Zhu, Johannes de Fine Licht, Zhenhao He, Runbin Shi, Cedric Renggli, Shuai Zhang, Theodoros Rekatsinas, Torsten Hoefler, Gustavo Alonso. Co-Design Hardware and Algorithm for Vector Search. The International Conference for High Performance Computing, Networking, Storage and Analysis, ACM, 2023. [<a href= "https://dl.acm.org/doi/abs/10.1145/3581784.3607045">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[SC'2023]</font></b> &nbsp; Shunde Li, Zongguo Wang, Lingkun Bu, Jue Wang, Zhikuang Xin, <b><font color="#002366">Shigang Li</font></b>, Yangang Wang, Yangde Feng, Peng Shi, Yun Hu, Xuebin Chi. ANT-MOC: Scalable Neutral Particle Transport Using 3D Method of Characteristics on Multi-GPU Systems. The International Conference for High Performance Computing, Networking, Storage and Analysis, ACM, 2023. (<font color="#FF0000">Best Paper Finalist, Best Student Paper Finalist</font>) [<a href= "https://dl.acm.org/doi/10.1145/3581784.3607063">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[SC'2023]</font></b> &nbsp; Yumeng Shi, Ningming Nie, Shunde Li, Jue Wang, Kehao Lin, Chunbao Zhou, <b><font color="#002366">Shigang Li</font></b>, Kehan Yao, Yangde Feng, Yan Zeng, Fang Liu, Yangang Wang, Yue Gao. Large-Scale Simulation of Structural Dynamics Computing on GPU Clusters. The International Conference for High Performance Computing, Networking, Storage and Analysis, ACM, 2023. [<a href= "https://dl.acm.org/doi/10.1145/3581784.3607082">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[MLSys'2023]</font></b> &nbsp; Kazuki Osawa*, <b><font color="#002366">Shigang Li*</font></b>, and Torsten Hoefler. PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices. The 6th Conference on Machine Learning and Systems, 2023. [<a href= "https://proceedings.mlsys.org/paper_files/paper/2023/file/dd064459e9ef4100671ba326f0f96f2b-Paper-mlsys2023.pdf">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[IPDPS'2023]</font></b> &nbsp; Daning Cheng, <b><font color="#002366">Shigang Li</font></b>, Yunquan Zhang. Asynch-SGBDT: Train Stochastic Gradient Boosting Decision Trees in an Asynchronous Parallel Manner. In the 37th IEEE International Parallel and Distributed Processing Symposium, 2023. [<a href= "https://ieeexplore.ieee.org/document/10177412">Paper</a>]
</li>

<li> <span><b><font color="#002366">[PPoPP'2023]</font></b> &nbsp; Kehao Lin, Chunbao Zhou, Yan Zeng, Ningming Nie, Jue Wang, <b><font color="#002366">Shigang Li</font></b>, Yangde Feng, Yangang Wang, Kehan Yao, Tiechui Yao, Jilin Zhang, Jian Wan. A Scalable Hybrid Total FETI Method for Massively Parallel FEM Simulations. In Proceedings of the 28th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, 2023. [<a href= "https://dl.acm.org/doi/abs/10.1145/3572848.3577517">Paper</a>] 
</span></li>


<li>
<span><b><font color="#002366">[SC'2022]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Kazuki Osawa, Torsten Hoefler. Efficient Quantized Sparse Matrix Operations on Tensor Cores. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2022. (<font color="#FF0000">Best Paper Finalist</font>) [<a href= "https://dl.acm.org/doi/10.5555/3571885.3571934">Paper</a>][<a href= "https://sc22.supercomputing.org/presentation/?id=pap265&sess=sess154">Talk</a>][<a href= "./files/sc22_Magicube_ShigangLi.pdf">Slides</a>][<a href= "https://github.com/Shigangli/Magicube">Code</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[SC'2022]</font></b> &nbsp; Torsten Hoefler, Tommaso Bonato, Daniele De Sensi, Salvatore Di Girolamo, <b><font color="#002366">Shigang Li</font></b>, Marco Heddes, Jon Belk, Deepak Goel, Miguel Castro, Steve Scott. HammingMesh: A Network Topology for Large-Scale Deep Learning. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2022. (<font color="#FF0000">Best Reproducibility Advancement Award, CACM Research Highlights</font>)[<a href= "https://arxiv.org/abs/2209.01346v1">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[ICS'2022]</font></b> &nbsp; Oliver Rausch, Tal Ben-Nun, Nikoli Dryden, Andrei Ivanov, <b><font color="#002366">Shigang Li</font></b>, and Torsten Hoefler. A Data-Centric Optimization Framework for Machine Learning. The 36th ACM International Conference on Supercomputing, 2022. [<a href= "https://arxiv.org/abs/2110.10802">Paper</a>][<a href= "https://github.com/spcl/daceml">Code</a>]
</span>
</li>

<li> <span><b><font color="#002366">[PPoPP'2022]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Torsten Hoefler. Near-Optimal Sparse Allreduce for Distributed Deep Learning. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, 2022. [<a href= "https://dl.acm.org/doi/10.1145/3503221.3508399">Paper</a>][<a href= "https://www.youtube.com/watch?v=Req6Cs17ur0">Talk</a>][<a href= "./files/oktopk-slides-shigangli.pdf">Slides</a>][<a href= "https://github.com/Shigangli/Ok-Topk">Code</a>] 
</span></li>

<li>
<span><b><font color="#002366">[SC'2021]</font></b> &nbsp; Daniele De Sensi, Salvatore Di Girolamo, Saleh Ashkboos, <b><font color="#002366">Shigang Li</font></b>, Torsten Hoefler. Flare: Flexible In-Network Allreduce. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, ACM, 2021. [<a href= "https://dl.acm.org/doi/10.1145/3458817.3476178">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[SC'2021]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Torsten Hoefler. Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, ACM, 2021. (<font color="#FF0000">Best Paper Finalist</font>) [<a href= "https://dl.acm.org/doi/abs/10.1145/3458817.3476145">Paper</a>][<a href= "https://www.youtube.com/watch?v=164XCpIZk6o&t=0s">Talk</a>][<a href= "./files/Chimera-SC21-shigangli-slides.pdf">Slides</a>][<a href= "https://github.com/Shigangli/Chimera">Code</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[NeurIPS'2021]</font></b> &nbsp; Giorgi Nadiradze, Amirmojtaba Sabour, Peter Davies, <b><font color="#002366">Shigang Li</font></b>, Dan Alistarh. Asynchronous Decentralized SGD with Quantized and Local Updates. In Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems, 2021. [<a href= "https://proceedings.neurips.cc/paper/2021/hash/362c99307cdc3f2d8b410652386a9dd1-Abstract.html">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[MLSys'2021]</font></b> &nbsp; Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, <b><font color="#002366">Shigang Li</font></b>, and Torsten Hoefler. Data Movement is All You Need: A Case Study on Optimizing Transformers. The 4th Conference on Machine Learning and Systems, 2021. (<font color="#FF0000">Outstanding Paper Award</font>, 5/52) [<a href= "https://proceedings.mlsys.org/paper/2021/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf">Paper</a>][<a href= "https://github.com/spcl/substation">Code</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[T SUSTAIN ENERG'2021]</font></b> &nbsp; Tiechui Yao, Jue Wang, Haoyan Wu, Pei Zhang, <b><font color="#002366">Shigang Li</font></b>, Ke Xu, Xiaoyan Liu, and Xuebin Chi. Intra-hour Photovoltaic Generation Forecasting based on Multi-source Data and Deep Learning Methods. IEEE Transactions on Sustainable Energy, 2021.
</span>
</li>

<li>
<span><b><font color="#002366">[PTRSA'2021]</font></b> &nbsp; Peter Grönquist, Chengyuan Yao, Tal Ben-Nun, Nikoli Dryden, Peter Dueben, <b><font color="#002366">Shigang Li</font></b>, and Torsten Hoefler. Deep Learning for Post-Processing Ensemble Weather Forecasts. Philosophical Transactions of the Royal Society A. [<a href= "https://arxiv.org/pdf/2005.08748.pdf">Paper</a>][<a href= "https://github.com/spcl/deep-weather">Code</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[TPDS'2021]</font></b> &nbsp; Daning Cheng#, <b><font color="#002366">Shigang Li#</font></b>, Hanping Zhang, Fen Xia, and Yunquan Zhang. Why Dataset Properties Bound the Scalability of Parallel Machine Learning Training Algorithms. IEEE Transactions on Parallel and Distributed Systems (2021). [<a href= "https://ieeexplore.ieee.org/abstract/document/9316159">Paper</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[TPDS'2021]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Tal Ben-Nun, Dan Alistarh, Salvatore Di Girolamo, Nikoli Dryden, and Torsten Hoefler. Breaking (Global) Barriers in Parallel Stochastic Optimization with Wait-Avoiding Group Averaging. IEEE Transactions on Parallel and Distributed Systems. [<a href= "https://ieeexplore.ieee.org/abstract/document/9271898">Paper</a>][<a href= "https://github.com/Shigangli/WAGMA-SGD">Code</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[JPDC'2020]</font></b> &nbsp; Daning Cheng, <b><font color="#002366">Shigang Li*</font></b>, Yunquan Zhang. WP-SGD: Weighted parallel SGD for distributed unbalanced-workload training system. Journal of Parallel and Distributed Computing 145 (2020): 202-216. [<a href= "./files/jpdc2020.pdf">Paper</a>]
</span>
</li>

<li> <span><b><font color="#002366">[PPoPP'2020]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Tal Ben-Nun, Salvatore Di Girolamo, Dan Alistarh, and Torsten Hoefler. Taming unbalanced training workloads in deep learning with partial collective operations. In Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pp. 45-61. 2020. (Acceptance rate: 23%, 28/121; <font color="#FF0000">Best Paper Nomination</font>, 5/28)
[<a href= "https://dl.acm.org/doi/10.1145/3332466.3374528">Paper</a>][<a href= "https://www.youtube.com/watch?v=CvoreY0Guws&t=0s">Talk</a>][<a href= "https://github.com/Shigangli/eager-SGD">Code</a>]
</span></li>

<li>
<span><b><font color="#002366">[JAMES'2020]</font></b> &nbsp; He Zhang, Minghua Zhang, ..., <b><font color="#002366">Shigang Li</font></b>, et al. CAS‐ESM 2: Description and climate simulation performance of the Chinese Academy of Sciences (CAS) Earth System Model (ESM) Version 2.  Journal of Advances in Modeling Earth Systems (2020): e2020MS002210.
</span>
</li>

<li>
<span><b><font color="#002366">[IPDPS'2020]</font></b> &nbsp; Hang Cao, Liang Yuan, He Zhang, Baodong Wu, <b><font color="#002366">Shigang Li</font></b>, Pengqi Lu, Yunquan Zhang, Yongjun Xu, and Minghua Zhang. A Highly Efficient Dynamical Core of Atmospheric General Circulation Model based on Leap-Format. In 2020 IEEE International Parallel and Distributed Processing Symposium, pp. 95-104. IEEE, 2020. [<a href= "./files/ipdps2020.pdf">Paper</a>]
</li>


<li>
<span><b><font color="#002366">[SC'2019]</font></b> &nbsp; Kun Li, Honghui Shang, Yunquan Zhang, <b><font color="#002366">Shigang Li</font></b>, Baodong Wu, Dong Wang, Libo Zhang, Fang Li, Dexun Chen, and Zhiqiang Wei. OpenKMC: a KMC design for hundred-billion-atom simulation using millions of cores on Sunway Taihulight. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, p. 68. ACM, 2019. (Acceptance rate: 22.7%, 78/344)
</span>
</li>

<li>
<span><b><font color="#002366">[ICTAI'2019]</font></b> &nbsp; Daning Cheng, Hanping Zhang, Fen Xia, <b><font color="#002366">Shigang Li</font></b>, and Yunquan Zhang. Using Gradient based multikernel Gaussian Process and Meta-acquisition function to Accelerate SMBO. In 2019 IEEE 31st International Conference on Tools with Artificial Intelligence, pp. 440-447. IEEE, 2019.
</span>
</li>

<li>
<span><b><font color="#002366">[JSUPERCOMPUT'2019]</font></b> &nbsp; Kun Li, <b><font color="#002366">Shigang Li*</font></b>, Shan Huang, Yifeng Chen, and Yunquan Zhang. FastNBL: fast neighbor lists establishment for molecular dynamics simulation based on bitwise operations. The Journal of Supercomputing (2019): 1-20.
</span>
</li>

<li>
<span><b><font color="#002366">[ISPA'2019]</font></b> &nbsp; Kun Li, <b><font color="#002366">Shigang Li</font></b>, Bei Wang, Yifeng Chen, and Yunquan Zhang. swMD: Performance Optimizations for Molecular Dynamics Simulation on Sunway Taihulight. In 2019 IEEE International Symposium on Parallel & Distributed Processing with Applications, pp. 511-518. IEEE, 2019.
</span>
</li>

<!--
<li>
<span><b><font color="#002366">[ICPADS'2018]</font></b> Baodong Wu, <b><font color="#002366">Shigang Li*</font></b>, Hang Cao, Yunquan Zhang, He Zhang, Junmin Xiao, and Minghua Zhang. <font color="#0000A0">AGCM3D: A Highly Scalable Finite-Difference Dynamical Core of Atmospheric General Circulation Model Based on 3D Decomposition</font>. In 2018 IEEE 24th International Conference on Parallel and Distributed Systems, pp. 355-364. IEEE, 2018 (Corresponding author).
</span>
</li>
-->

<li>
<span><b><font color="#002366">[TPDS'2018]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Yunquan Zhang, and Torsten Hoefler. Cache-oblivious MPI all-to-all communications based on Morton order. IEEE Transactions on Parallel and Distributed Systems 29, no. 3 (2018): 542-555. [<a href= "https://ieeexplore.ieee.org/abstract/document/8091010">Paper</a>][<a href= "https://www.youtube.com/watch?v=w52qBZ2sSoE">Talk</a>][<a href= "https://github.com/Shigangli/COMPI">Code</a>]
</span>
</li>


<li>
<span><b><font color="#002366">[ICPP'2018]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Baodong Wu, Yunquan Zhang, Xianmeng Wang, Jianjiang Li, Changjun Hu, Jue Wang, Yangde Feng, and Ningming Nie. Massively scaling the metal microscopic damage simulation on sunway taihulight supercomputer. In Proceedings of the 47th International Conference on Parallel Processing, pp. 1-11. 2018. [<a href= "https://dl.acm.org/doi/abs/10.1145/3225058.3225064">Paper</a>][<a href= "./files/ICPP-2018-slides.pdf">Slides</a>]
</span>
</li>


<li>
<span><b><font color="#002366">[ICPP'2018]</font></b> &nbsp; Junmin Xiao, <b><font color="#002366">Shigang Li</font></b>, Baodong Wu, He Zhang, Kun Li, Erlin Yao, Yunquan Zhang, and Guangming Tan. Communication-avoiding for dynamical core of atmospheric general circulation model. In Proceedings of the 47th International Conference on Parallel Processing, pp. 1-10. 2018.
</span>
</li>

<li>
<span><b><font color="#002366">[JPDC'2018]</font></b> &nbsp; Zhihao Li, Haipeng Jia, Yunquan Zhang, Shice Liu, <b><font color="#002366">Shigang Li</font></b>, Xiao Wang, and Hao Zhang. Efficient parallel optimizations of a high-performance SIFT on GPUs. Journal of Parallel and Distributed Computing 124 (2019): 78-91.
</span>
</li>

<li>
<span><b><font color="#002366">[ICPADS'2018]</font></b> &nbsp; Baodong Wu, <b><font color="#002366">Shigang Li*</font></b>, Hang Cao, Yunquan Zhang, He Zhang, Junmin Xiao, and Minghua Zhang. AGCM3D: A Highly Scalable Finite-Difference Dynamical Core of Atmospheric General Circulation Model Based on 3D Decomposition. In 2018 IEEE 24th International Conference on Parallel and Distributed Systems, pp. 355-364. IEEE, 2018. (Corresponding Author) [<a href= "./files/AGCM3D-icpads.pdf">Paper</a>][<a href= "./files/AGCM3D-slides.pdf">Slides</a>]
</span>
</li>


<li>
<span><b><font color="#002366">[PPoPP'2017]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Yunquan Zhang, and Torsten Hoefler. Cache-oblivious MPI all-to-all communications on many-core architectures. Poster, ACM SIGPLAN Notices 52, no. 8 (2017): 445-446. [<a href= "https://dl.acm.org/doi/10.1145/3155284.3019025">Paper</a>]
</span>
</li>


<li>
<span><b><font color="#002366">[CPC'2017]</font></b> &nbsp; Changjun Hu, Xianmeng Wang, Jianjiang Li, Xinfu He, <b><font color="#002366">Shigang Li</font></b>, Yangde Feng, Shaofeng Yang, and He Bai. Kernel optimization for short-range molecular dynamics. Computer Physics Communications 211 (2017): 31-40.
</li>


<li>
<span><b><font color="#002366">[CPC'2017]</font></b> &nbsp; Baodong Wu, <b><font color="#002366">Shigang Li*</font></b>, Yunquan Zhang, and Ningming Nie. Hybrid-optimization strategy for the communication of large-scale Kinetic Monte Carlo simulation. Computer Physics Communications 211 (2017): 113-123. (Corresponding Author)
</li>


<li>
<span><b><font color="#002366">[TACO'2016]</font></b> &nbsp; Yunquan Zhang, <b><font color="#002366">Shigang Li*</font></b>, Shengen Yan*, and Huiyang Zhou. A cross-platform spmv framework on many-core architectures. ACM Transactions on Architecture and Code Optimization (TACO) 13, no. 4 (2016): 1-25. (Corresponding Author) [<a href= "https://dl.acm.org/doi/10.1145/2994148">Paper</a>][<a href= "https://github.com/Shigangli/SpMV-on-Many-Core">Code</a>]
</span>
</li>

<li>
<span><b><font color="#002366">[PIEEE'2016]</font></b> &nbsp; Yunquan Zhang, Ting Cao, <b><font color="#002366">Shigang Li</font></b>, Xinhui Tian, Liang Yuan, Haipeng Jia, and Athanasios V. Vasilakos. Parallel processing systems for big data: a survey. Proceedings of the IEEE 104, no. 11 (2016): 2114-2136.
</li>

<li>
<span><b><font color="#002366">[SCIS'2015]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, ChangJun Hu, JunChao Zhang, and YunQuan Zhang. Automatic tuning of sparse matrix-vector multiplication on multicore clusters. Science China Information Sciences 58, no. 9 (2015): 1-14.
</li>

<li>
<span><b><font color="#002366">[HPCC'2015]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Yunquan Zhang, Chunyang Xiang, and Lei Shi. Fast convolution operations on many-core architectures. In 2015 IEEE 17th International Conference on High Performance Computing and Communications, pp. 316-323. IEEE, 2015.[<a href= "https://ieeexplore.ieee.org/document/7336182">Paper</a>][<a href= "./files/HPCC15-slides.pdf">Slides</a>]
</li>

<li>
<span><b><font color="#002366">[CCGrid'2015]</font></b> &nbsp; Xiaomin Zhu, Junchao Zhang, Kazutomo Yoshii, <b><font color="#002366">Shigang Li</font></b>, Yunquan Zhang, and Pavan Balaji. Analyzing MPI-3.0 process-level shared memory: A case study with stencil computations. In 2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, Workshop, pp. 1099-1106. IEEE, 2015.
</li>

<li>
<span><b><font color="#002366">[CLUSTER COMPUT'2014]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Torsten Hoefler, Chungjin Hu, and Marc Snir. Improved MPI collectives for MPI processes in shared address spaces. Cluster computing 17, no. 4 (2014): 1139-1155.
</li>

<li>
<span><b><font color="#002366">[HPDC'2013]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Torsten Hoefler, and Marc Snir. NUMA-aware shared-memory collective communication for MPI. In Proceedings of the 22nd international symposium on High-performance parallel and distributed computing, pp. 85-96. 2013. (Acceptance rate: 15%, 20/131; <font color="#FF0000">Best Paper Nomination</font>, 3/20) [<a href= "https://dl.acm.org/doi/10.1145/2493123.2462903">Paper</a>]
</li>

<li>
<span><b><font color="#002366">[PDP'2013]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Jingyuan Hu, Xin Cheng, and Chongchong Zhao. Asynchronous work stealing on distributed memory systems. In 2013 21st Euromicro International Conference on Parallel, Distributed, and Network-Based Processing, pp. 198-202. IEEE, 2013.
</li>

<li>
<span><b><font color="#002366">[ICA3PP'2011]</font></b> &nbsp; <b><font color="#002366">Shigang Li</font></b>, Shucai Yao, Haohu He, Lili Sun, Yi Chen, and Yunfeng Peng. Extending synchronization constructs in openMP to exploit pipeline parallelism on heterogeneous multi-core. In International Conference on Algorithms and Architectures for Parallel Processing, Workshop, pp. 54-63. Springer, Berlin, Heidelberg, 2011.
</li>

<!--
<li>
<span><b><font color="#002366">[ICCS'2011]</font></b> &nbsp; Yunfeng Peng, Changjun Hu, Chongchong Zhao, <b><font color="#002366">Shigang Li</font></b>, and Shucai Yao. Management of Non-functional Attributes of Parallel Components. Procedia Computer Science 4 (2011): 461-470.
</li>

<li>
<span><b><font color="#002366">[ICA3PP'2010]</font></b> &nbsp; Qian Cao, Changjun Hu, Haohu He, Xiang Huang, and <b><font color="#002366">Shigang Li</font></b>. Support for OpenMP tasks on cell architecture. In International Conference on Algorithms and Architectures for Parallel Processing, Workshop, pp. 308-317. Springer, Berlin, Heidelberg, 2010.
</li>
-->
</font>
</ul>


<!--
<A NAME="Projects"><h2>Projects</h2></A>
<font size="4"> 
<ul>
<li> Project Leader &#151; MPI Model Extension and Performance Optimization for Many-Core Clusters, National Natural Science Foundation of China</li>
<li> Project Leader &#151; MPI Communication Optimization for Irregular Parallel Algorithms, State Key Laboratory of Computer Architecture Foundation </li>
<li> Technical Principal &#151; High Performance Deep Learning Library Development on CPU and GPU Architectures, IT Company Foundation </li>
<li> Technical Principal &#151; Large-Scale Deep Learning Training System on Heterogeneous Parallel Machines, IT Company Foundation </li>
</ul>
</font>
-->

<!--
<A NAME="People"><h2>People</h2></A>
<font size="4"> 
<ul>
<li><font color="blue"><b>Lab Director</b></font></li>
      &nbsp; &nbsp; Shigang Li
<li><font color="blue"><b>Postdocs</b></font></li>
      &nbsp; &nbsp; Xueying Wang
<li><font color="blue"><b>PhD students</b></font></li>
<li><font color="blue"><b>Master students</b></font></li>
</ul>
</font>
-->

<A NAME="Teaching"><h2>Teaching</h2></A>
<font size="4"> 
<ul>
<li>Lecture &#151; Beijing University of Posts and Telecommunications, <a href="">Computer Organization and Architecture</a>, from Spring 2025 </li>
<li>Lecture &#151; Beijing University of Posts and Telecommunications, <a href="">Computer Architecture</a>, from Spring 2024 </li>
<li>Lecture &#151; Beijing University of Posts and Telecommunications, <a href="">Digital Logic and Systems</a>, from Autumn 2023 </li>
<li>Exercise session &#151; ETH Zurich, <a href="https://spcl.inf.ethz.ch/Teaching/2022-pp/">Parallel Programming</a>, Spring 2022</li>
<li>Exercise session &#151; ETH Zurich, <a href="https://spcl.inf.ethz.ch/Teaching/2021-pp/">Parallel Programming</a>, Spring 2021</li>
<li>Exercise session &#151; ETH Zurich, <a href="https://spcl.inf.ethz.ch/Teaching/2019-pp/">Parallel Programming</a>, Spring 2019</li>
<!--
<li>Teaching Assistant &#151; ETH Zurich, <a href="http://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2019W&ansicht=KATALOGDATEN&lerneinheitId=132388&lang=en">Numerical Methods for CSE, Autumn 2019</a>, C++/Eigen programming </li>
-->
<li>Exercise session &#151; ETH Zurich, <a href="http://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2019W&ansicht=KATALOGDATEN&lerneinheitId=132388&lang=en">Numerical Methods for CSE, C++/Eigen programming</a>, Autumn 2019</li>
<li>Exercise session &#151; ETH Zurich, <a href="https://metaphor.ethz.ch/x/2018/hs/401-0663-00L/">Numerical Methods for CSE, C++/Eigen programming</a>, Autumn 2018</li>
</ul>
</font>


<A NAME="Academic Services"><h2>Academic Services</h2></A>
<font size="4"> 
<ul>
<li>TPC Track Chair, HPC China 2024, 2023</li>
<li>Publicity Chair (Europe), PPoPP 2023</li>
<li>Publications Chair, IISWC 2020</li>
<li>Workshop Co-Chair, ICS 2018</li>
<li>Local Co-Chair, CCF SYS 2025</li>
<li>Program Committee Member, SC 2025, 2023, 2022, 2021</li>
<li>Program Committee Member, PPoPP 2026, 2022</li>
<li>Program Committee Member, IEEE Cluster 2025, 2024, 2022, 2021</li>
<li>Program Committee Member, IPDPS 2024, 2021, 2018, 2017</li>
<li>Program Committee Member, ICPP 2023, 2022, 2017</li>
<li>Program Committee Member, ICPADS 2022, 2018</li>
<li>Program Committee Member, APPT 2025</li>
<li>Program Committee Member, NPC 2025, 2024</li>
<li>Program Committee Member, HPC Asia 2024, 2021, 2020, 2019, 2018</li>
<li>Program Committee Member, HPC China 2022, 2021, 2019, 2018, 2017, 2016</li>
<li>Program Committee Member, ChinaSys 2024</li>
<li>Program Committee Member, DPCS 2023, 2022</li>
<li>Program Committee Member, SBAC-PAD 2022, 2020, 2016 (ERC)</li>
<li>Program Committee Member, PMAM 2023, 2022, 2021</li>
<li>Program Committee Member, HP3C 2020, 2019, 2018</li>
<li>Program Committee Member, INFOCOMP 2022, 2021, 2020</li>
<li>Research Posters Member, SC 2023</li>
<!--<br>-->
<li>Associate Editor of Cluster Computing (CLUS) - Springer</li>
<li>Youth Editor of CCF THPC</li>
<li>Reviewer of IEEE Transactions on Parallel and Distributed Systems (TPDS)</li>
<li>Reviewer of IEEE Transactions on Services Computing (TSC)</li>
<li>Reviewer of IEEE Transactions on Big Data (TBD)</li>
<li>Reviewer of IEEE Transactions on Network Science and Engineering</li>
<li>Reviewer of IEEE Transactions on Circuits and Systems II: Express Briefs</li>
<li>Reviewer of Journal of Parallel and Distributed Computing (JPDC) - Elsevier</li>
<li>Reviewer of Journal of Supercomputing - Springer</li>
<li>Reviewer of Concurrency and Computation: Practice and Experience</li>
<li>Reviewer of Mobile Networks and Applications – Springer</li>
<li>Program Committee Member, IEEE TPDS Special Section on Parallel and Distributed Computing Techniques for AI, ML, and DL, 2020</li>
</ul>
</font>




<br />
<div style="margin: 0 auto; width: 30%;">
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=OQycp-yjEK5gN9sgEY4lWurb2iNqLH-NJ6nqMQPRCYY&cl=ffffff&w=a"></script>
</div>

<!--
<script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=OQycp-yjEK5gN9sgEY4lWurb2iNqLH-NJ6nqMQPRCYY"></script>
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5ymalyc7n8v&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script> 
-->

</body>
</html>
